<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-CL61JF076E"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-CL61JF076E');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SSM networks and S4</title>
    <link rel="stylesheet" href="../../../style.css">
    <link rel="stylesheet" href="https://use.typekit.net/znl5vhc.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="icon" href="https://www.chus.space/favicon.ico?v=2"/>
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <meta property="og:type" content="article">
    <!-- MODIFY FOREACH-->
    <!-- entry title -->
    <meta property="og:title" content="SSM networks and S4 - Chus Antonanzas">
    <!-- entry description -->
    <meta name="description" content="Description">
    <meta property="og:description" content="Description">
    <!-- entry name / entry image TODO REPLACE-->
    <meta property="og:image" content="https://www.chus.space/static/entryname/ssms.jpg"> 
    <!-- entry name -->
    <meta property="og:url" content="https://www.chus.space/blog/2024/ssm_2_networks"> 
    <meta property="og:site_name" content="Chus Antonanzas">
    <meta property="og:locale" content="en_US">
    <script>
      window.MathJax = {
          tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']],
              displayMath: [['$$', '$$']],
              packages: ['base', 'ams'],
              tags: 'ams', // Enables AMS-style numbering
              tagSide: 'right',
              tagIndent: '0.8em'
          },
          options: {
              ignoreHtmlClass: 'tex2jax_ignore',
              processHtmlClass: 'tex2jax_process'
          }
      };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
  <script src="../../../src/progress_bar.js"></script>
  <script src="../../../src/dynamic_share_url.js"></script>
  <script src="../../../src/ssm_source.js"></script>
</head>

<body>

  <header>
      <nav id="navbar">
          <div class="logo-and-title">
              <a href="../../index.html" style="display: flex; align-items: center; text-decoration: none; color: inherit;">
                  <h1>Chus Antonanzas</h1>
              </a>
          </div>
          <ul>
              <li><a href="../../about.html">About</a></li>
              <li><a href="../../contact.html">Contact</a></li>
          </ul>
      </nav>
  </header>

  <div id="progress-container">
    <div id="progress-bar"></div>
  </div>

  <div class="content-container entry">
    <article>

    <div class="blog-header">
        <h1>State Space Models (2): networks and S4</h1>
        <p class="tagline">In this series, we explore how State Space Models (SSMs) work, their history, some intuitions, and recent advances, including Mamba and Mamba 2. </p>
        <div class="header-date-links">
            <div class="header-date">
                <span>June 10th, 2024</span>
            </div>
            <div class="social-links">
                <div id="copy-notification" style="display: none; /* other styling */">URL copied</div>
                <a target="_blank" aria-label="Twitter"> <i class="fab fa-twitter-square"></i> </a>
                <a href="#" onclick="copyToClipboard(window.location.href)" aria-label="Copy link"> <i class="fas fa-link"></i></a>
            </div>
        </div>
        <hr>
    </div>

    <section id="Intro">
      <h2>Intro</h2>

      <p>This is the second part of the State Space Model series! In this post, we are see how SSMs were first incorporated into neural networks. Then, we are 
        going to see the fundamental flaws in this first approach (spoiler, there are 2), and how S4 approached them. To give some context, this is where
      we are in the series:</p>
  
      <ul>
        <li><span class="bold"><a href="../ssm_1_context/index.html">Part 1: Motivation and introduction to SSMs</a></span>
          <ul>We go through where SSMs are right now, how they appeared into the machine learning panorama, and what they are.</ul>
        </li>
        
        <li><span class="bold">Part 2: SSM networks and Structured State Spaces</span>
          <ul>We cover how SSMs were incorporated into neural networks, their shortcomings, and S4, an immediate successor and improvement.</ul>
        </li>
        <li><span class="bold"><a href="../ssm_3_selectivity/index.html">Part 3: Selective State Spaces (Mamba)</a></span>
          <ul>We present Selective State Spaces, which added to SSMs the capacity of focusing on different parts of the input (yes, like attention).</ul>
        </li>
        <li><span class="bold"><a href="../ssm_4_duality/index.html">Part 4: State Space Duality (Mamba 2)</a></span>
          <ul>What's the relationship between SSMs and the attention mechanism? They are actually the same! What does this imply?</ul>
        </li>
      </ul>

    <p>
       As a reminder, here are the continuous

      $$
      \begin{align}
        h'(t) &= \mathbf{A} h(t) + \mathbf{B} x(t) \label{eq:ssm} \\
        y(t) &= \mathbf{C}h(t) \nonumber
      \end{align}
      $$

      and discrete

      $$
        \begin{align}
         h_t &= \bar{\mathbf{A}} h_{t-1} + \bar{\mathbf{B}} x_t \label{eq:ssm_dis} \\
         y_t &= \bar{\mathbf{C}} h_t \nonumber
        \end{align}
      $$

      SSM representations that we presented in the previous post. Let's formalize the dimensions of their parameters, as we need more rigor from this point
      forward:
    </p>

    <p>A discrete SSM defines a map $x \in \mathbb{R}^{T} \rightarrow y \in \mathbb{R}^{T}$, where $T$ is the time dimension. That is, $x_t$ and $y_t$ are scalars.
      We also have the hidden state vector $h_t \in \mathbb{R}^{N}$, where $N$ is the state size, state dimension, state expansion factor, or even more.
      Parameters $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ can be thought of as tensors of shapes $\mathbf{A} \in \mathbb{R}^{(N, N)}$, 
      $\mathbf{B} \in \mathbb{R}^{N}$ and $\mathbf{C} \in \mathbb{R}^{N}$. The discretization parameter $\Delta \in \mathbb{R}$ is a scalar.
    </p>

    <p>Phew. With that out of the way, let's get going. Before we see how SSMs can be used in neural nets, 
      we will see how the ways of computing them was characterized.</p>

    </section>

    <section id="The Linear State-Space Layer">
      <h2>The Linear State-Space Layer</h2>

      <p>The vanilla SSM that we presented in the last post was generalized in 2021 by ((todo cite combining...)), which they called 
        Linear State-Space Layer (LSSL).
        They characterized the LSSL in terms of its parameters, $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ and $\Delta$, and importantly by
        providing multiple interpretations of the model. These interpretations, or views, allowed us to define the same LSSL in different ways. </p>

      <h3>The three LSSL views</h3>
      <p>The authors of LSSLs argued that they could be viewed in three ways: </p>

      <ol>
        <li>A continuous-time view, where the LSSL is just equation $\ref{eq:ssm}$.</li>
        <li>A recurrent view, where the LSSL was discretized like in equation $\ref{eq:ssm_dis}$ and its operations were performed iteratively.</li>
        <li>A convolutional view, where the LSSL was also discretized and its operations were defined as a convolution with a filter.</li>
      </ol>

      <p>The first view we have already explored. If you noticed, the second and third views only differ from one another by how the output is computed. 
        Let's address this, because it's <span class="bold">essential</span>. </p>

    </section>

    <section id="The recurrent view">
      <h2>The recurrent view</h2>

      <p>How would you implement the mechanical SSM simulation that we presesented at the end of the previous post? Well, one could write down the parameters
        and create a function that given the previous hidden state and the current input, computes the next hidden state. Then, we could iterate this function
        over all inputs, each one depending on the last one. This is the recurrent view! It's just implementing equation $\ref{eq:ssm_dis}$ naively.</p>

      TODO recurrent view.

      <p>This view is quite efficient! The amount of memory and computation used is the same in each step: we only store the parameters, previous state and current
        input. The downside is, it's sequential, so for a massive number of inputs, we will need to go one by one, without parallelization (a small lie, we will revisit
        this later). </p>

    </section>

    <section id="The convolutional view">
      <h2>The convolutional view</h2>

      <p>What's the other view? Well, if we pay closer attention to the recurrent view and write down how each step is computed, we can unroll the whole sequence.
      Following equation $\ref{eq:ssm_dis}$, and assuming that $h_{-1} = 0$:

      $$
      \begin{align}
      x_0 &= \bar{\mathbf{B}} x_0, & x_1 &= \bar{\mathbf{A}} \bar{\mathbf{B}} x_0 + \bar{\mathbf{B}} x_1, & x_2 &= \bar{\mathbf{A}}^2 \bar{\mathbf{B}} x_0 + \bar{\mathbf{A}} \bar{\mathbf{B}} x_1 + \bar{\mathbf{B}} x_2, & \ldots \nonumber \\ 
      y_0 &= \bar{\mathbf{C}} \bar{\mathbf{B}} x_0, & y_1 &= \bar{\mathbf{C}} \bar{\mathbf{A}} \bar{\mathbf{B}} x_0 + \bar{\mathbf{C}} \bar{\mathbf{B}} x_1, & y_2 &= \bar{\mathbf{C}} \bar{\mathbf{A}}^2 \bar{\mathbf{B}} x_0 + \bar{\mathbf{C}} \bar{\mathbf{A}} \bar{\mathbf{B}} x_1 + \bar{\mathbf{C}} \bar{\mathbf{B}} x_2, & \ldots \nonumber
      \end{align}
      $$
         
      </p>

      <p>We can see, or you can believe me, that this follows a pattern:</p>

      $$
      y_k = \bar{\mathbf{C}} \bar{\mathbf{A}}^k \bar{\mathbf{B}} x_0 + \bar{\mathbf{C}} \bar{\mathbf{A}}^{k-1} \bar{\mathbf{B}} x_1 + \ldots + \bar{\mathbf{C}} \bar{\mathbf{A}} \bar{\mathbf{B}} x_{k-1} + \bar{\mathbf{C}} \bar{\mathbf{B}} x_k
      $$
      

      <p>It can be shown that, given this expression, we can compute the whole output sequence $y$ with just an operation: the convolution. In particular,
        the convolution of the input sequence $x$ with what is called the <span class="bold">SSM convolution kernel</span>, or filter, 
        $\bar{\mathbf{K}} \in \mathbb{R}^{T}$:
      </p>

      \begin{equation*}
      y = \bar{\mathbf{K}} \ast x
      \label{eq:conv}
      \end{equation*}

      \begin{equation*}
      \bar{\mathbf{K}} = (\bar{\mathbf{C}} \bar{\mathbf{B}}, \bar{\mathbf{C}} \bar{\mathbf{A}} \bar{\mathbf{B}}, \ldots, \bar{\mathbf{C}} \bar{\mathbf{A}}^{L-1} \bar{\mathbf{B}})
      \end{equation*}

      <p>We can also illustrate the operation:</p>

      TODO ilustraci√≥n de la convolution.

      <p>Now, as you may know, convolution is parallelizable, which is a huge plus versus the recurrent view. Moreover, we can go further and, using the
        discrete convolution theorem, perform the operation by multiplying the Fast Fourier Transforms of the input $x$ and the filter $\bar{\mathbf{K}}$ and then applying
        and inverse FFT to the result, making the operation more efficient. On the other hand, though, we see that the SSM filter
        is just as long as the sequence ($T$); its huge. This is a big downside, as we need to store all the parameters in memory.
      </p>

      TODO ilustracion de FFT, multiplicacion and back

      <p> So, these two views allow us to compute the same output in different ways. The recurrent view is memory and compute efficient, but sequential,
      while the convolutional view is memory heavy but can be parallelizable. Are there situations when one is better than the other? Well, yes! TODO contiunue.</p>

    </section>

  </article>

  </div>

</body>

</html>
