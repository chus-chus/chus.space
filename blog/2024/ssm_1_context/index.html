<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CL61JF076E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-CL61JF076E');
  </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intro to SSMs</title>
    <link rel="stylesheet" href="../../../style.css">
    <link rel="stylesheet" href="https://use.typekit.net/znl5vhc.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="icon" href="https://www.chus.space/favicon.ico?v=2"/>
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <meta property="og:type" content="article">
    <!-- MODIFY FOREACH-->
    <!-- entry title -->
    <meta property="og:title" content="An introduction to State Space Models - Chus Antonanzas">
    <!-- entry description -->
    <meta name="description" content="Description">
    <meta property="og:description" content="Description">
    <!-- entry name / entry image TODO REPLACE-->
    <meta property="og:image" content="https://www.chus.space/static/entryname/ssms.jpg"> 
    <!-- entry name -->
    <meta property="og:url" content="https://www.chus.space/blog/2024/ssm_1_context">
    <meta property="og:site_name" content="Chus Antonanzas">
    <meta property="og:locale" content="en_US">
    <script>
      window.MathJax = {
          tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']],
              displayMath: [['$$', '$$']],
              packages: ['base', 'ams'],
              tags: 'ams', // Enables AMS-style numbering
              tagSide: 'right',
              tagIndent: '0.8em'
          },
          options: {
              ignoreHtmlClass: 'tex2jax_ignore',
              processHtmlClass: 'tex2jax_process'
          }
      };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
  <script src="../../../src/progress_bar.js"></script>
  <script src="../../../src/dynamic_share_url.js"></script>
  <script src="../../../src/ssm_source.js"></script>
</head>

<body>

  <header>
      <nav id="navbar">
          <div class="logo-and-title">
              <a href="../../index.html" style="display: flex; align-items: center; text-decoration: none; color: inherit;">
                  <h1>Chus Antonanzas</h1>
              </a>
          </div>
          <ul>
              <li><a href="../../about.html">About</a></li>
              <li><a href="../../contact.html">Contact</a></li>
          </ul>
      </nav>
  </header>

  <div id="progress-container">
    <div id="progress-bar"></div>
  </div>

  <div class="content-container entry">
    <article>

    <div class="blog-header">
        <h1>State Space Models (1): context</h1>
        <p class="tagline">In this series, we explore how State Space Models (SSMs) work, their history, some intuitions, and recent advances, including Mamba and Mamba 2. </p>
        <div class="header-date-links">
            <div class="header-date">
                <span>June 10th, 2024</span>
            </div>
            <div class="social-links">
                <div id="copy-notification" style="display: none; /* other styling */">URL copied</div>
                <a target="_blank" aria-label="Twitter"> <i class="fab fa-twitter-square"></i> </a>
                <a href="#" onclick="copyToClipboard(window.location.href)" aria-label="Copy link"> <i class="fas fa-link"></i></a>
            </div>
        </div>
        <hr>
    </div>

    <section id="Intro">
      <h2>Intro</h2>
      <p>Welcome to the first part of the State Space Model series! Motivated by their recent popularity and my own interest in the topic, I decided to explore State Space Models (SSMs) in depth. 
        In the series, we will cover how SSMs work, how they became competitive with the state-of-the-art, all up to the most recent advances mainly through a visualization and exploration lens.</p>
      <p>I have tried to make the content engaging but through. If you have any questions or suggestions, please let me know!</p>
  
      <p>The series is divided into these parts:</p>
        <ul>
          <li><span class="bold">Part 1: Motivation and introduction to SSMs</span>
            <ul>You are here. We go through where SSMs are right now, how they appeared into the machine learning panorama, and what they are.</ul>
          </li>
          
          <li><span class="bold"><a href="../ssm_2_networks/index.html">Part 2: SSM networks and Structured State Spaces</a></span>
            <ul>We cover how SSMs were incorporated into neural networks, their shortcomings, and S4, an immediate successor and improvement.</ul>
          </li>
          <li><span class="bold"><a href="../ssm_3_selectivity/index.html">Part 3: Selective State Spaces (Mamba)</a></span>
            <ul>We present Selective State Spaces, which added to SSMs the capacity of focusing on different parts of the input (yes, like attention).</ul>
          </li>
          <li><span class="bold"><a href="../ssm_4_duality/index.html">Part 4: State Space Duality (Mamba 2)</a></span>
            <ul>What's the relationship between SSMs and the attention mechanism? They are actually the same! What does this imply?</ul>
          </li>
        </ul>
    </section>

    
    
    <section id="State Space Models of today">
      <h2>State Space Models of today</h2>

      <p>The Transformer (the architecture) supremacy has been active for quite a few years now, and the ML discourse is mainly still centered around them. 
        Many variations, tricks and hacks have been added on top of the original model, but the core mechanics stay the same: attention. Attention has allowed Transformers
      to stay as the state of the art in many tasks (mainly related to language). Of course, the Transformer is not the only architecture that is of use today, because 
      in some domains it's not quite. Still, it's dominance is clear! </p>

      <p> But yes, other areas are also actively innovating. MLPs, CNNs, RNNs and other arquitectures are constantly "receiving" updates, but can't really catch up. 
        Researchers are trying to come up with architectures that can support our views on how the future of AI will look like. One of them, State Space Models, has been
        stirring up some attention by the community; two of the proponents, Tri Dao and Albert Gu, are making some strong points.</p>

      TODO insert grafico con nuevas arquitecturas e innovaciones presentadas a lo largo del tiempo.

      <p>Before jumping to what they are and how they work, are SSMs currently competitive? </p>

      <h3>SSMs vs the world</h3>

      <p>There are many tasks agains which models are evaluated, and many metrics to perform those evaluations. But we can take a look at the most popular ones.
        How does the performance of the SSM stack against the state of the art? (TODO add note: Mamba 2 has been recently presented, but it does not add performance).
      </p>

      TODO BLABLA
      TODO graficos con comparaciones en performance. 

      <p>On top of their task performance, SSMs have properties that make them appealing. Among others, linear scaling in memory and compute [TODO DOUBLE CHECK]. 
      This makes them look promising, yes, but we need to approach this critically and judge for ourselves. To this end, let's go back to the fundamentals.
      </p>
    </section>

    <section id="State Space Models of yesterday">
      <h2>State Space Models of yesterday</h2>

      <p>SSMs are not actually recent, they actually go back all they way to the mid 20-th century. 
        In the 1960’s, Wiener’s and Kalman’s work on control theory introduced the concept of SSM for modeling linear systems. 
        And while SSMs started being used in many fields such as computational neuroscience, researchers were only able to 
        incorporate them effectively into deep learning until very recently for theoretical reasons. </p>
      <p>In 2020, because Transformers could not model long sequence data <span class="italic">efficiently</span> and in the search of alternatives, the 
        Long-Range Arena benchmark was presented. It focused on evaluating model quality under long context scenarios: data 
        with Long Range Dependencies (LRD). In the paper, the authors found that for modeling that kind of data, it was hard 
        to do better than vanilla Transformers.</p>
      <p>The next year, Albert Gu and others showed that although vanilla SSM blocks could be used in conjuntion with other deep 
        learning techniques, they struggled to efficiently model LRD data. But, in the following years, the theoretical framework was 
        improved drastically with techniques and ideas that we will explore in the next posts. Before going into that, though, 
        what <span class="italic">are</span> SSMs?</p>

      <h3>The State Space Model</h3>

      <p>The idea of the SSM is the following: model each step in a sequence given its respective input and information about the previous state of the sequence.
        In particular, given a instant $t$ and an input signal $x(t)$, an SSM maps it to an output $y(t)$ while keeping track of a state representation $h(t)$:
      </p>

      $$
      \begin{align}
        h'(t) &= \mathbf{A} h(t) + \mathbf{B} x(t) \label{eq:ssm} \\
        y(t) &= \mathbf{C}h(t) + \mathbf{D}x(t) \nonumber
      \end{align}
      $$

      <p>Where $\mathbf{A}, \mathbf{B}, \mathbf{C}$ and $\mathbf{D}$ are weight matrices. Usually, the term $\mathbf{D}x(t)$ is not taken into account, 
        because it can be equated to a skip connection and does not interact with the hidden state. We will assume this from now on. </p>

      <p>Equation \ref{eq:ssm} basically defines two things:
        <ol>
        <li>how the hidden state changes with respect to the current input and hidden state (the state representation) values. </li>
        <li>how the output depends on the hidden state. </li>
        </ol>
      
      Graphically:

      TODO GRAFICO SSM

      TODO comentarios sobre grafico

      <p>A few notes about the components:
        <ul>
          <li>$\mathbf{A}$ defines how the previous hidden state influences the change of state. </li>
          <li>$\mathbf{B}$ defines how the input influences the change of state. </li>
          <li>$\mathbf{C}$ defines how the output depends on the hidden state. </li>
        </ul>
      </p>

      TODO GRAFICAMENTE
      
      </p>

      <p>Note how equation \ref{eq:ssm} works over the continuous space! Well, when working with signals, such a representation is common. In our case, and if we want
        SSMs to work with sequences like their colleagues, this continuous representation is not completely accurate: sequences are discrete! Mathematically, discrete functions
      cannot be differentiated, so equation \ref{eq:ssm} is not valid. We need to <span class="bold">discretize</span> the SSM. </p>

    </section>

    <section id="Discretized State Space Models">
      <h2>Discretized State Space Models</h2>

      The discretization of a signal can be interpreted as sampling. For example, given the continuous function $x(t)$, we sample the element $x_t$ with time interval 
      $\Delta $ ($x_t = x(\Delta t)$). This gives us an input sequence $(x_1, x_2, ...)$.

      TODO grafico de discretización de una señal.

      <p>In order to derive the discretized expression of the SSM, we observe how it behaves given a discrete input sequence. But, because the SSM cannot work
        with discrete data, we interpret the discrete sequence as continuous! There are actually quite a few ways to do this, but a common one is the TODO PONER BIEN zero order holdout. 

        TODO grafico de discretización de una señal con zero order holdout. 
      </p>

      <p>We can study how the different parameters of the SSM behave under this reconstructed data. The "discretized" versions are annotated with an overhead line, 
        and are dependent on the step size $\Delta$, which is a learnable parameter, and fixed discretization functions (like zero-order holdout). So, 

        $$
        \begin{align}
        \bar{\mathbf{A}} &= f_A(\Delta, \mathbf{A}) \nonumber \\
        \bar{\mathbf{B}} &= f_B(\Delta, \mathbf{A}, \mathbf{B}) \nonumber \\
        \end{align}
        $$
      </p>

      <p> Matrix $\mathbf{C}$ is not discretized because it only depends on $h(t)$, which in turn depends on the discretized matrices $\bar{\mathbf{A}}$ and $\bar{\mathbf{B}}$.
        Now, because the results are not trivial, I won't go into details about why the discretized expression is like it is. I'm presenting these formulas only because you might happen
        to see them in other posts. In the case of $f$ being zero-order holdout, the discretized parameters can be computed as:

        $$
        \begin{align}
          \bar{\mathbf{A}} &= e^{\Delta\mathbf{A}} \nonumber \\
          \bar{\mathbf{B}} &= (\Delta \mathbf{A})^{-1} (e^{\Delta\mathbf{A}}-\mathbf{I}) \cdot \Delta \mathbf{B} \nonumber
        \end{align}
        $$

        The SSM can naturally work with sequence data! A large $\Delta$ will make the model take more into account the current input The discretized version of the SSM is:

        $$
        \begin{align}
         h_t &= \bar{\mathbf{A}} h_{t-1} + \bar{\mathbf{B}} x_t \label{eq:ssm_dis} \\
         y_t &= \bar{\mathbf{C}} h_t \nonumber
        \end{align}
        $$

      </p>
        
      <p>Now, the state equation refers to the update of the actual state, not its rate of change as in equation $\ref{eq:ssm}$. Note that the continous representation of A, B, etc from equation \ref{eq:ssm} is still what is actually learned, and is later discretized. Recent works (2023) have proposed 
        that this discretization step can be skipped and parametrize A and B directly. This is a bit easier to understand, but won't be actually used in the series. </p>

      <p>To wrap up, for sequence data, an SSM will have a set of parameters $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$, a discretization of the first 
        two, $\bar{\mathbf{A}}$ and $\bar{\mathbf{B}}$, and will transform an input sequence $(x_1, x_2, ...)$ into a sequence $(y_1, y_2, ...)$ by recursively applying
        formula $\ref{eq:ssm_dis}$.</p>

        TODO grafico de como primero se coge la representation continua, luego se discretiza, y con eso se manda la señal, obteniendo secuencia de salida y.

    </section>

    <section id="A mechanical example">
      <h2>A mechanical example</h2>

      <p>Now that we know what they are and how they are formulated, let's see a SSM in action! We take an example from the amazing post TODO CITE ilustrated s4. </p>

      <p>Imagine we have a spring attached to a wall and supported by a horizontal surface. At the end of the spring there is a mass $m$ attached, like a weight. 
        Over time ($t$) we pull on the mass with a force $u(t)$, making the spring bounce back and forth, dependent on its stiffness $k$ and the friction $b$. 
        We constantly measure the position of the mass $y(t)$. </p>

      TODO diagrama del problema.

      <p>In order to model this system, we start from how we know that force is mass times acceleration. In a differential equation, this is:</p>

      \begin{equation}
      u(t) - b\cdot y'(t) - k\cdot y(t) = m\cdot y''(t)
      \label{eq:mechanics_diff}
      \end{equation}

      <p>That is, the friction $b$ impacts speed and the stiffness $k$ impacts position, all subtracting from the force we apply. All that is equal to the mass of the weight
       times its acceleration. </p>
       <p>We can express the system like an SSM: the rates of change of the system (speed and acceleration) are presented in equation
       $\ref{eq:ssm}$ as $h'(t)$, meaning that the hidden state $h(t)$ is actually just the position and speed. 
       We also know that the input of the system is its current position, and the output is its next position. So, we just arrange the parameters:</p>

      $$
      \begin{align}
      A &= \begin{bmatrix}
      0 & 1 \\
      -\frac{k}{m} & -\frac{b}{m}
      \end{bmatrix} \nonumber \\

      B &= \begin{bmatrix}
      0 \\
      \frac{1}{m}
      \end{bmatrix} \nonumber \\

      C &= \begin{bmatrix}
      1 & 0
      \end{bmatrix} \nonumber
      \end{align} 
      $$

      <p>And there you have it. If we discretize the system as explained before, we can simulate it as an SMM. </p>

      TODO mechanical simulation.

    </section>

    <section id="Next up">
      <h2>Next up</h2>

      We know their context, what they are and how they work.
      In the next <a href="../ssm_2_discretization/index.html">post</a>, we are going to see how SSMs were incorporated into 
      neural networks and made effective via some cool math trickery.

      <div id="sample_viz"></div>

      <div id="reference-list"></div>

    </section>

  </article>

  </div>

<script>document.addEventListener('DOMContentLoaded', function() {
  const svg = d3.select('#sample_viz').append('svg')
      .attr('width', 800)
      .attr('height', 600);

  // sample bar chart
  const data = [4, 8, 15, 16, 23, 42];
  svg.selectAll('rect')
      .data(data)
      .enter().append('rect')
      .attr('x', (d, i) => i * 100)
      .attr('y', d => 600 - d * 10)
      .attr('width', 50)
      .attr('height', d => d * 10)
      .attr('fill', 'steelblue');
});

// -------------------------------------------------- References --------------------------------------------------
const references = [
  {id: "ref1", author: "Author Name", year: "Year", title: "Title of the Document", publisher: "Publisher", url: "url"},
  {id: "ref2", author: "Author Name", year: "Year", title: "Title of the Document", publisher: "Publisher", url: "url"}
  // Add more references as needed
];

document.addEventListener('DOMContentLoaded', function() {
  const refList = document.getElementById('reference-list');
  references.forEach(ref => {
      const listItem = document.createElement('li');
      listItem.id = ref.id;
      listItem.innerHTML = `${ref.author}. (${ref.year}). <i>${ref.title}</i>. ${ref.publisher}. <a href="${ref.url}">link</a>`;
      refList.appendChild(listItem);
  });
});
</script>

</body>

</html>
