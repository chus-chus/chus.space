<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CL61JF076E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-CL61JF076E');
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Selectivity & Mamba</title>
  <link rel="stylesheet" href="../../../style.css">
  <link rel="stylesheet" href="https://use.typekit.net/znl5vhc.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="icon" href="https://www.chus.space/favicon.ico?v=2"/>
  <meta name="robots" content="index, follow">
  <meta name="googlebot" content="index, follow">
  <meta property="og:type" content="article">
  <!-- MODIFY FOREACH-->
  <!-- entry title -->
  <meta property="og:title" content="Selective State Spaces: Mamba - Chus Antonanzas">
  <!-- entry description -->
  <meta name="description" content="Description">
  <meta property="og:description" content="Description">
  <!-- entry name / entry image TODO REPLACE-->
  <meta property="og:image" content="https://www.chus.space/static/entryname/ssms.jpg"> 
  <!-- entry name -->
  <meta property="og:url" content="https://www.chus.space/blog/2024/ssm_3_selectivity"> 
  <meta property="og:site_name" content="Chus Antonanzas">
  <meta property="og:locale" content="en_US">
  <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            packages: ['base', 'ams'],
            tags: 'ams', // Enables AMS-style numbering
            tagSide: 'right',
            tagIndent: '0.8em'
        },
        options: {
            ignoreHtmlClass: 'tex2jax_ignore',
            processHtmlClass: 'tex2jax_process'
        }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
  <script src="../../../src/progress_bar.js"></script>
  <script src="../../../src/dynamic_share_url.js"></script>
  <script src="../../../src/ssm_source.js"></script>
</head>

<body>
  <header>
    <nav id="navbar">
        <div class="logo-and-title">
            <a href="../../../index.html" style="display: flex; align-items: center; text-decoration: none; color: inherit;">
                <h1>Chus Antonanzas</h1>
            </a>
        </div>
        <ul>
            <li><a href="../../../about/index.html">About</a></li>
            <li><a href="../../../contact/index.html">Contact</a></li>
        </ul>
    </nav>
</header>

  <div id="progress-container">
    <div id="progress-bar"></div>
  </div>

  <div class="content-container entry">
    <article>

    <div class="blog-header">
        <h1>State Space Models (3): Mamba & Mamba 2</h1>
        <p class="tagline">In this series, we explore how State Space Models (SSMs) work, their history, some intuitions, and recent advances, including Mamba and Mamba 2. </p>
        <div class="header-date-links">
            <div class="header-date">
            <!-- TODO adjust date -->
                <span>June 10th, 2024</span>
            </div>
            <div class="social-links">
                <div id="copy-notification" style="display: none; /* other styling */">URL copied</div>
                <a target="_blank" aria-label="Twitter"> <i class="fab fa-twitter-square"></i> </a>
                <a href="#" onclick="copyToClipboard(window.location.href)" aria-label="Copy link"> <i class="fas fa-link"></i></a>
            </div>
        </div>
        <hr>
    </div>
    
    <section id="Intro">
      <h2>Intro</h2>

      <p>⚠️ Under construction!</p>

      <p>Welcome back to the SSM series! This is part 3 of 3, where we finally talk about Mamba and Mamba 2.
        If you haven't read the previous parts, I recommend you do so before continuing.</p>
      
      <ul>
        <li><span class="bold"><a href="../ssm_1_context/index.html">Part 1: Motivation and introduction to SSMs</a></span>
          <ul>We go through where SSMs are right now, how they appeared into the machine learning panorama, and what they are.</ul>
        </li>
        
        <li><span class="bold"><a href="../ssm_2_networks/index.html">Part 2: SSM networks and Structured State Spaces</a></span>
          <ul>We cover how SSMs were incorporated into neural networks, their shortcomings, and S4, an immediate successor and improvement.</ul>
        </li>
        <li><span class="bold">Part 3: State Space Selectivity (Mamba) and Duality (Mamba 2)</span>
          <ul>We present Selective State Spaces and State Space Duality. Selectivity allows SSMs to focus on different parts of the input, like attention.
            Speaking of, what's the relationship between SSMs and attention? They are actually the same! What does this imply?
          </ul>
        </li>
      </ul>

    <p>If you read the previous posts, by now you should understand:</p>

    <ul>
      <li>how a basic SSM block works.</li>
      <li>how SSM blocks can be represented in different ways. Each interpretation (recurrent, convolutional), although equivalent in functionality, had different characteristics and was preferable under some circumstances.</li>
      <li>how SSM blocks could be stacked and organized to create a sequence mapper with the same signature as a common neural network.</li>
      <li>what enables SSM blocks to have long term memory (the HiPPO $\mathbf{A}$ matrix).</li>
      <li>how the memory footprint problem of the convolutional view of an SSM is mitigated (structured state spaces).</li>
    </ul>

    <p>All in all, with what we know right now, one could very well create a performant (both in predictive capabilities and resource requirements) 
      SSM-based model capable of competing with similar, more traditional methods.</p>
    
    <p>There is a particular technique that is now ubiquitous, though, and which we have not used for our comparisons yet. Let’s address the elephant in the room: 
      can all this even really begin to compete with what the attention mechanism can do?</p>
      
    </section>

    <section id="Attention, please">
      <h2>Attention, please</h2>
      <p>Although something similar to attention existed before 2017, <span class="italic">Vaswani et al.</span> made it popular with <a href="#attention">Attention Is All You Need</a>.
        The transformer caught the world by storm and changed the deep learning panorama. Why? Because attention enabled the model to be able to 
        select which parts of the input, or even intermediate steps, could be useful for each prediction. This is called selectivity. Now, 
        selectivity meant that the behavior of the model would not always the same, in the sense that different operations would be performed 
        by the model depending on the input. To predict the word that comes after the sequence “All felines are cool, and so is my _” the model 
        would not consider all words in the sequence uniformly useful, but rather it would probably understand that the sequence refers to felines 
        in particular and thus predict “cat” as the next word. There are a gazillion resources on attention, so I won’t go into more detail here.</p>
      </p>
      <p>Now, if we refer to our current formulation of an SSM block, we see how all matrices ($\bar{\mathbf{A}}$, $\bar{\mathbf{B}}$, $\bar{\mathbf{C}}$) are static:
        
      $$
      \begin{align}
        h_t &= \bar{\mathbf{A}} h_{t-1} + \bar{\mathbf{B}} x_t \label{eq:ssm_dis} \\
        y_t &= \bar{\mathbf{C}} h_t \nonumber
      \end{align}
      $$

      That is, whatever the input $h_t$ is, the same operations are always performed. Thus, selectivity cannot occur right now, 
        as there is now way for our models to discern what is useful and what is not. More formally, they are <span class="italic">linear time invariant</span> (LTI).
      </p>

      <p>So, if SSMs are to become equally as capable as our current state of the art models, they need to be selective!</p>

    </section>
    <section id="Mamba: selectivity in SSMs">
      <h2>Selectivity in SSMs</h2>
      <h3>Motivation: selection as a means of compression</h3>
      <p>When a vanilla attention mechanism is used, all key and value representations of all parts of an input are saved in memory. Naturally, this much information 
        allows for a powerful way of selecting what is useful and what is not, at every moment. However, this also means that the memory footprint of the model is very large (many advances have been made).
        This is effective but inefficient, as nothing is compressed.</p>

        <p>On the other hand, when a vanilla SSM block, or a RNN, is used, the hidden state compresses all input information, up to the current part, into a fixed-size vector. This is very efficient, but not really effective, 
          as all parts of the input all considered equally useful. This is efficient but not effective, as there is indiscriminate compression.</p>

        <p>Now, because SSM blocks use a fixed-size hidden state, there will always be compression. For an effective compression, though, the model needs to be able to select what is useful and what is not:
          selection as a means of compression. Thus, a fixed hidden state coupled with selection will help us better explore this effectiveness-efficiency tradeoff.</p>

      <h3>Parameters as functions of the input</h3>
      <p>How do we actually make SSM blocks selective? A related question for the reader is: what parameters, from equation $\ref{eq:ssm_dis}$, influence 
        how information propagates and how the output is constructed? The anwsers are (1) all matrices that update the hidden state ($\bar{\mathbf{A}}$ and 
        $\bar{\mathbf{B}}$), and (2) the $\bar{\mathbf{C}}$ matrix. If they are to be able to be selective, that means that they need to behave differently 
        depending on the input. That is, those parameters need to be input-dependant.</p>

      <p>For this, the Mamba authors propose a simple solution: create parametrised (trainable) functions that map the input in each timestep to the corresponding parameter matrices.
        These functions exist for each parameter that needs to be selective. For example, if we want the $\bar{\mathbf{B}}$ matrix to be selective, 
        we train, as part of the SSM block, the linear projection $s_B(x) = $Linear${}_N(x)$ that maps the input $x$ to the $\mathbf{B}$ matrix. This way, the $\bar{\mathbf{B}}$ matrix 
        will have a different structure depending on the input and will selectively interact with information.</p>

        <p>Note how, in the previous paragraph, the linear map outputs the undiscretized matrix! To be precise, the Mamba paper proposes to make 
          $\mathbf{B}, \mathbf{C}, \Delta$ functions of the input:

          $$
          \begin{align}
          \mathbf{B} &= s_B(x) \nonumber \\
          \mathbf{C} &= s_C(x) \nonumber \\
          \Delta &= \tau_{\Delta}(p + s_{\Delta}(x)) \nonumber
          \end{align}
          $$

          where $p$ is a fixed parameter and:
        
          $$
          \begin{align}
            s_{\Delta}(x) &= \text{Broadcast}_T(\text{Linear}_1(x)) \nonumber \\
            s_B(x) &= \text{Linear}_N(x) \nonumber \\
            s_C(x) &= \text{Linear}_N(x) \nonumber \\
            \tau_{\Delta} &= \text{Softplus} \nonumber
            
          \end{align}
          $$

          where $T$ is the size of input sequence (time dimension), and $N$ is the state size.
          Skip the latex if you want, but the main idea is that the matrices are functions of the input, and that the input is mapped to the matrices by linear functions.
          The $\Delta$ matrix is constructed like that because, as the authors show, it's actually a nice generalisation of RNN gating.
          
          <p>An important comment to make is why the $\mathbf{A}$ matrix is not function of the input, but a fixed parameter just like before.
            Look at equation $\ref{eq:ssm_dis}$ and think about the roles of each matrix. $\bar{\mathbf{A}}$ enables memorization of the hidden state (it's a HiPPO matrix),
          $\bar{\mathbf{B}}$ determines how the input influences the state, and $\bar{\mathbf{C}}$ how the state influences the output. The purpose of selectivity is
          for the model to be able to focus on different parts of the input to output a prediction. Thus, how the previous hidden state influences the current one is not
          something that should be selective, but rather a fixed operation. If you don't find this argument compelling enough, also note how the only way $\mathbf{A}$
          interacts with the system is via $\bar{\mathbf{A}} = e^{\Delta \mathbf{A}}$ (the discretization, which we address in <a href="../ssm_1_context/index.html">post 1</a> of the series). Thus, one could argue that selectivity in $\Delta$
          is enough for $\bar{\mathbf{A}}$ to be selective.</p>
        

        <h3>The importance of implementation</h3>
        <p>With the change of making some parameters input dependent, the ability to express an SSM as a convolution (see previous post) is lost: the kernel is not fixed anymore.
          Thus, we only have the recurrent view of an SSM block, which is not as efficiently computed as the convolutional view (when expressed naively).
          A big contribution of the Mamba paper is the implementation of the selective SSM block, with which they address the following problems:</p>
          <ol>
            <li><span class="bold">Parallelisation of the recurrent view.</span><p></p></li>
            <li><span class="bold">Memory limitations.</span></li>
          </ol>

          <p>Let's address them.</p>

          </section>

          <!--<p>If the authors had not included an efficient implementation of the selective SSM block, Mamba would have been impractical for real world use.</p>-->

          <section id="Mamba: making it practical">
            <h2>Mamba: making it practical</h2>
            <p>The Mamba paper is a wonderful piece of research: many of the ideas are not new, but a special kind of mind is required for bringing them together and implement them in harmony. 
              If the authors had not included an efficient implementation of the selective SSM block, Mamba would have been impractical for real world use. 
              So, in this section, I want to briefly lay down the intuition behind what I consider two of the most relevant 
              implementation details. For further reading, I really
          recommend TODO cite https://jameschen.io/jekyll/update/2024/02/12/mamba.html#fnref:kvcache ,and </p>

            <h3>The Blelloch parallel scan</h3>
            <p>For the training of Mamba to be efficient, we need to parallelise it. The Blelloch parallel scan algorithm has been (relatively recently) used for parallelizing RNNs (todo cite Eric Martin and Chris Cundy. “Parallelizing Linear Recurrent Neural Nets Over Sequence Length”. In: The Interna-
              tional Conference on Learning Representations (ICLR). 2018.), and if you
              have been paying attention, something in you might tell you that it may be possible to also apply it to State Space Models. Well, that's exactly what happened!</p>
              
              <p>As its name suggests, the parallel scan algorithm is used to perform an operation called a 
              <span class="italic">scan</span> in parallel. In short, the scan iteratively and cumulatively applies an operator to a list.
              TODO fig scan example</p>

              <p>This scan operation is actually how recurrent models compute hidden states:</p>

              TODO fig rnn scan

              <p>But an RNN, or Mamba, performs more operations than just sums! Well, the parallel scan algorithm doesn't only work with the sum operator, but with all binary (input two numbers, output one) and associative (the order does not matter) 
                operators. So, if we can design an operator for Mamba, we can parallelise the scan (this is simplified, because the generalisation for recurrences is a bit more specific, but I refer you to the
                mentioned source if you need more details). As a spoiler, the resulting operator is a way to express the computation of $\overline{\mathbf{A}_t}$ and $\overline{\mathbf{B}(x_t)}x_t$ at any point in 
                time (in the sequence) $t$. With them, follow equation $\ref{eq:ssm_dis}$ to see how $h_t$ is computed. This is sufficient for applying the Blelloch algorithm, thus parallelising 
                the training of Mamba.</p>

              <p>As a sidenote, this parallelisation is also useful at inference time, but not as much given that requests will come in an asynchronous manner. 
                It's a problem of batching and scheduling, then. </p>

              <h3>Hardware-aware computing</h3>
              <p>Nowadays, a lot of software is developed without taking any target hardware into account. In spite of this, for certain applications, there are plenty of benefits of knowing 
                how the hardware your application is going to run in works.</p>

              <p>You might recognize one of the Mamba authors, Tri Dao, by his work on FlashAttention todo cite. FlashAttention is an implementation of the attention mechanism that drastically reduces the amount
                of memory transfers between High-Bandwidth Memory and SRAM, from where the processor units pull the data for performing operations. They way this is done is out of the scope of this post, but the main idea is that because Attention is memory-bound (more time moving things around
                than actually doing computations), the less memory transfers, the faster the model. This is a hardware-aware implementation.</p>
                
              <p>The authors recognised that Mamba was memory-bound and did the same: reduce the amount of memory transfers. The usual flow for a GPU program is:
                <ol>
                  <li>Load data from HBM to SRAM.</li>
                  <li>Compute.</li>
                  <li>Store results from SRAM to HBM.</li>
                </ol>
                A naive implementation of an SSM block would discretize, update the hidden state, and then the output, all in separate steps (multiple loops of the flow above). Instead, we discretize, 
                update the hidden state, and compute the output all in one loop. This is called <span class="italic">kernel fusion</span>, a fancy word for combining operations in a single GPU program.
              </p>

              <p>Another technique that they use to make the selective SSM block efficient is <span class="italic">recomputation</span>, which means to avoid storing objects that are
                needed later on (such in the backward pass) in memory and instead compute them again. When an operation is clearly memory-bound, this makes everything go faster.</p>

              <p>In conclusion, the selective SSM block is implemented efficiently mainly by:
                <ul>
                  <li>Parallelising the scan operation with the Blelloch algorithm.</li>
                  <li>Using hardware-aware computing techniques such as kernel fusion and recomputation.</li>
                </ul>
                
                Including more details would be repeating what's said in the paper, so if you're not sated, please refer to it.</p>

                <h3>The Mamba block</h3>
                <p>Just as we described in <a href="../ssm_2_networks/index.html">post 2</a>, the SSM block is not the only ingredient for a succesful model.
                We saw how one could combine multiple SSM blocks with traditional operations such as softmax or linear projections to build a "neural network"
                with SSMs as a core. Mamba is no different: in the paper, the selective SSM block is combined with multiple other operations to form a Mamba block.
              
              todo fig    </p>

          </section>

          <section id="Mamba 2">
            <h2>Mamba 2</h2>

          </section>

         
          
        

          


        

    </section>

    <section id="Next up & references">
      <h2>Next up</h2>

      <h3>References</h3>
      <p><ul id="reference-list"></ul></p>

    </section>

  </article>

  </div>

  <script>

    // -------------------------------------------------- References --------------------------------------------------
    const references = [
      {id: "attention", author: "Vaswani et al", year: "2017", title: "Attention Is All You Need", url: "https://arxiv.org/abs/1706.03762"}
    ];
    
    document.addEventListener('DOMContentLoaded', function() {
      const refList = document.getElementById('reference-list');
      references.forEach(ref => {
          const listItem = document.createElement('li');
          listItem.id = ref.id;
          listItem.innerHTML = `${ref.author}. (${ref.year}). <i>${ref.title}</i>. <a href="${ref.url}">link</a>`;
          refList.appendChild(listItem);
      });
    });
    </script>

</body>

</html>
