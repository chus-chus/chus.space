<!DOCTYPE html>
<html lang="en">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CL61JF076E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-CL61JF076E');
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Mamba 1 & 2 | Chus</title>
  <link rel="stylesheet" href="../../../style.css">
  <link rel="stylesheet" href="https://use.typekit.net/znl5vhc.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="icon" href="https://www.chus.space/favicon.ico?v=2"/>
  <meta name="robots" content="index, follow">
  <meta name="googlebot" content="index, follow">
  <meta property="og:type" content="article">
  <!-- MODIFY FOREACH-->
  <!-- entry title -->
  <meta property="og:title" content="Mamba 1 & 2 - Chus Antonanzas">
  <!-- entry description -->
  <meta name="description" content="Description">
  <meta property="og:description" content="Description">
  <!-- entry name / entry image TODO REPLACE-->
  <meta property="og:image" content="https://www.chus.space/static/entryname/ssms.jpg"> 
  <!-- entry name -->
  <meta property="og:url" content="https://www.chus.space/blog/2024/ssm_3_mambas"> 
  <meta property="og:site_name" content="Chus Antonanzas">
  <meta property="og:locale" content="en_US">
  <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            packages: ['base', 'ams'],
            tags: 'ams', // Enables AMS-style numbering
            tagSide: 'right',
            tagIndent: '0.8em'
        },
        options: {
            ignoreHtmlClass: 'tex2jax_ignore',
            processHtmlClass: 'tex2jax_process'
        }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!--<script src="https://d3js.org/d3.v6.min.js"></script>-->
  <script src="../../../src/progress_bar.js"></script>
  <script src="../../../src/dynamic_share_url.js"></script>
</head>

<body>
  <header>
    <nav id="navbar">
        <div class="logo-and-title">
            <a href="../../../index.html" style="display: flex; align-items: center; text-decoration: none; color: inherit;">
                <h1>Chus Antonanzas</h1>
            </a>
        </div>
        <ul>
            <li><a href="../../../about/index.html">About</a></li>
            <li><a href="../../../contact/index.html">Contact</a></li>
        </ul>
    </nav>
</header>

  <div id="progress-container">
    <div id="progress-bar"></div>
  </div>

  <div class="content-container entry">
    <article>

    <div class="blog-header">
        <h1>State Space Models (3): Mamba & Mamba 2</h1>
        <p class="tagline">In this series, we explore how State Space Models (SSMs) work, their history, some intuitions, and recent advances, including Mamba and Mamba 2. </p>
        <div class="header-date-links">
            <div class="header-date">
            <!-- TODO adjust date -->
                <span>September 27th, 2024</span>
            </div>
            <div class="social-links">
                <div id="copy-notification" style="display: none; /* other styling */">URL copied</div>
                <a target="_blank" aria-label="Twitter"> <i class="fab fa-twitter-square"></i> </a>
                <a href="#" onclick="copyToClipboard(window.location.href)" aria-label="Copy link"> <i class="fas fa-link"></i></a>
            </div>
        </div>
        <hr>
    </div>
    
    <section id="Intro">
      <h2>Intro</h2>

      <p>Welcome back to the SSM series! This is part 3 of 3, where we finally talk about Mamba and Mamba 2.
        If you haven't read the previous parts, I recommend you do so before continuing.</p>
      
      <ul>
        <li><span class="bold"><a href="../ssm_1_context/index.html">Part 1: Motivation and introduction to SSMs</a></span>
          <div>We go through where SSMs are right now, how they entered the machine learning landscape, and what they are.</div>
        </li>
        
        <li><span class="bold"><a href="../ssm_2_networks/index.html">Part 2: SSM neural networks and Structured State Spaces</a></span>
          <div>We cover how SSMs were incorporated into neural networks, their shortcomings, and S4, an immediate successor and improvement.</div>
        </li>
        <li><span class="bold">Part 3: State Space Selectivity (Mamba) and Duality (Mamba 2)</span>
          <div>We present Selective State Spaces and State Space Duality. Selectivity allows SSMs to focus on different parts of the input, like Attention.
            Speaking of, what's the relationship between SSMs and attention? In fact, they are surprisingly similar! What does this imply?
          </div>
        </li>
      </ul>

    <p>If you read the previous posts, by now you should understand:</p>

    <ul>
      <li>how a basic SSM block works.</li>
      <li>how SSM blocks can be represented in different ways. Each interpretation (recurrent, convolutional) is equivalent in functionality but has different characteristics, making one preferable over the other in certain contexts.</li>
      <li>how SSM blocks could be stacked and organized to create a sequence mapper with the same signature as a common neural network.</li>
      <li>what enables SSM blocks to have long term memory (the HiPPO $\mathbf{A}$ matrix).</li>
      <li>how the memory footprint problem of the convolutional view of an SSM is mitigated (structured state spaces).</li>
    </ul>

    <p>In summary, based on what we know now, one could very well create a performant (both in predictive capabilities and resource requirements) 
      SSM-based model capable of competing with similar, more traditional methods.</p>
    
    <p>There is a particular technique that is now ubiquitous, though, and which we have not used for our comparisons yet. Let’s address the elephant in the room: 
      can all this even really begin to compete with what the attention mechanism can do?</p>
      
    </section>

    <section id="Attention, please">
      <h2>Attention, please</h2>
      <p>Although something similar to attention existed before 2017, <span class="italic">Vaswani et al.</span> made it popular with <a href="#attention">Attention Is All You Need</a>.
        The transformer caught the world by storm and changed the deep learning panorama. Why? Because attention enabled the model to be able to 
        select which parts of the input, or even intermediate steps, could be useful for each prediction. This is called selectivity. Now, 
        selectivity meant that the behavior of the model would not always the same, in the sense that different operations would be performed 
        by the model depending on the input. To predict the word that comes after the sequence “All felines are cool, and so is my _” the model 
        would not consider all words in the sequence uniformly useful, but rather it would probably understand that the sequence refers to felines 
        in particular and thus predict “cat” as the next word. There are a million resources on attention, so I won’t go into more detail here.
      </p>
      <p>Now, if we refer to our current formulation of an SSM block, we see how all matrices ($\bar{\mathbf{A}}$, $\bar{\mathbf{B}}$, $\bar{\mathbf{C}}$) are static:
        
      $$
      \begin{align}
        h_t &= \bar{\mathbf{A}} h_{t-1} + \bar{\mathbf{B}} x_t \label{eq:ssm_dis} \\
        y_t &= \bar{\mathbf{C}} h_t \nonumber
      \end{align}
      $$

      That is, whatever the input $h_t$ is, the same operations are always performed. Thus, selectivity cannot occur right now, 
        as there is no way for our models to discern what is useful and what is not. More formally, they are <span class="italic">linear time invariant</span> (LTI).
      </p>

      <p>So, if SSMs are to become equally as capable as our current state of the art models, they need to be selective!</p>

    </section>
    <section id="Mamba: selectivity in SSMs">
      <h2>Selectivity in SSMs</h2>
      <h3>Motivation: selection as a means of compression</h3>
      <p>When a vanilla attention mechanism is used, all key and value representations of all parts of an input are saved in memory. Naturally, this much information 
        allows for a powerful way of selecting what is useful and what is not, at every moment. However, this also means that the memory footprint of the model is very large (many advances have been made).
        This is effective but inefficient, as nothing is compressed.</p>

      <p>On the other hand, when a vanilla SSM block, or a RNN, is used, the hidden state compresses all input information, up to the current part, into a fixed-size vector. This is very efficient, but not really effective, 
        as all parts of the input are considered equally useful. This is efficient, but not effective, due to indiscriminate compression.</p>

      <p>Now, because SSM blocks use a fixed-size hidden state, there will always be compression. For an effective compression, though, the model needs to be able to select what is useful and what is not:
        selection as a means of compression. Thus, a fixed hidden state coupled with selection will help us better explore this effectiveness-efficiency tradeoff.</p>

      <h3>Parameters as functions of the input</h3>
      <p>How can we make SSM blocks selective? A related question for the reader is: what parameters, from equation $\ref{eq:ssm_dis}$, influence 
        how information propagates and how the output is constructed? The answers are (1) all matrices that update the hidden state ($\bar{\mathbf{A}}$ and 
        $\bar{\mathbf{B}}$), and (2) the $\bar{\mathbf{C}}$ matrix. If they are to be able to be selective, that means that they need to behave differently 
        depending on the input. That is, those parameters need to be input-dependant.</p>

      <p>For this, the Mamba authors propose a simple solution: create parametrised (trainable) functions that map the input in each timestep to the corresponding parameter matrices.
        These functions exist for each parameter that needs to be selective. For example, if we want the $\bar{\mathbf{B}}$ matrix to be selective, 
        we train, as part of the SSM block, the linear projection $s_B(x) = $Linear${}_N(x)$ that maps the input $x$ to the $\mathbf{B}$ matrix. This way, the $\bar{\mathbf{B}}$ matrix 
        will have a different structure depending on the input and will selectively interact with information.</p>

      <p>Note how, in the previous paragraph, the linear map outputs the undiscretized matrix! To be precise, the Mamba paper proposes to make 
        $\mathbf{B}, \mathbf{C}, \Delta$ functions of the input:

        $$
        \begin{align}
        \mathbf{B} &= s_B(x) \nonumber \\
        \mathbf{C} &= s_C(x) \nonumber \\
        \Delta &= \tau_{\Delta}(p + s_{\Delta}(x)) \nonumber
        \end{align}
        $$

        where $p$ is a fixed parameter and:
      
        $$
        \begin{align}
          s_{\Delta}(x) &= \text{Broadcast}_T(\text{Linear}_1(x)) \nonumber \\
          s_B(x) &= \text{Linear}_N(x) \nonumber \\
          s_C(x) &= \text{Linear}_N(x) \nonumber \\
          \tau_{\Delta} &= \text{Softplus} \nonumber
          
        \end{align}
        $$

        where $T$ is the size of the input sequence (time dimension), and $N$ is the state size.
        The main idea is that the matrices are functions of the input, and that the input is mapped to the matrices by linear functions.
        The $\Delta$ matrix is constructed like that because, as the authors show, it's actually a nice generalisation of RNN gating.
        
      <p>An important comment to make is why the $\mathbf{A}$ matrix is not function of the input, but a fixed parameter just like before.
        Look at equation $\ref{eq:ssm_dis}$ and think about the roles of each matrix. $\bar{\mathbf{A}}$ enables memorization of the hidden state (it's a HiPPO matrix),
      $\bar{\mathbf{B}}$ determines how the input influences the state, and $\bar{\mathbf{C}}$ how the state influences the output. The purpose of selectivity is
      for the model to be able to focus on different parts of the input. Thus, the influence of the previous hidden state on the current one should remain a fixed operation, rather than being selective.
      If you don't find this argument compelling enough, also note how the only way $\mathbf{A}$
      interacts with the system is via $\bar{\mathbf{A}} = e^{\Delta \mathbf{A}}$ (the discretization, which we address in <a href="../ssm_1_context/index.html">post 1</a> of the series). Thus, one could argue that selectivity in $\Delta$
      is enough for $\bar{\mathbf{A}}$ to be selective.</p>
      

      <h3>The importance of implementation</h3>
      <p>With the change of making some parameters input dependent, the ability to express an SSM as a convolution (see previous post) is lost: the kernel is not fixed anymore.
        Thus, we only have the recurrent view of an SSM block, which is not as efficiently computed as the convolutional view (when expressed naively).
        A big contribution of the Mamba paper is the implementation of the selective SSM block, with which they address the following problems:</p>

      <ol>
        <li><span class="bold">Parallelisation of the recurrent view.</span><p></p></li>
        <li><span class="bold">Memory limitations.</span></li>
      </ol>

      <p>Let's address them.</p>

    </section>

    <section id="Mamba: making it practical">
      <h2>Mamba: making it practical</h2>
      <p>The Mamba paper is a wonderful piece of research. While many of the ideas are not new, a special kind of mind is required to bring them together and implement them in harmony.
        Had the authors not included an efficient implementation of the selective SSM block, Mamba would have been impractical for real world use. 
        So, in this section, I want to briefly lay down the intuition behind what I consider two of the most relevant 
        implementation details. For further reading, I strongly recommend TODO cite https://jameschen.io/jekyll/update/2024/02/12/mamba.html#fnref:kvcache ,and </p>

      <h3>The Blelloch parallel scan</h3>
      <p>For the training of Mamba to be efficient, we need to parallelise it. The Blelloch parallel scan algorithm has been (relatively recently) used for parallelizing RNNs (todo cite Eric Martin and Chris Cundy. “Parallelizing Linear Recurrent Neural Nets Over Sequence Length”. In: The Interna-
        tional Conference on Learning Representations (ICLR). 2018.), and if you
        have been paying attention, something in you might tell you that it may be possible to also apply it to State Space Models. Well, that's exactly what happened!</p>
        
        <p>As its name suggests, the parallel scan algorithm is used to perform an operation called a 
        <span class="italic">scan</span> in parallel. In short, the scan iteratively and cumulatively applies an operator to a list.
        TODO fig scan example</p>

        <p>This scan operation is actually how recurrent models compute hidden states:</p>

        TODO fig rnn scan

        <p>But an RNN, or Mamba, performs more operations than just sums! Well, the parallel scan algorithm doesn't only work with the sum operator, but with all binary (input two numbers, output one) and associative (the order does not matter) 
          operators. So, if we can design an operator for Mamba, we can parallelise the scan (this is simplified, because the generalisation for recurrences is a bit more specific, but I refer you to the
          mentioned source if you need more details). As a spoiler, the resulting operator is a way to express how $\overline{\mathbf{A}_t}$ and $\overline{\mathbf{B}(x_t)}x_t$ are computed at any point in 
          time (in the sequence) $t$. With them, follow equation $\ref{eq:ssm_dis}$ to see how $h_t$ is computed. This is sufficient for applying the Blelloch algorithm, thus parallelising 
          the training of Mamba.</p>

        <p>As a sidenote, this parallelisation is also useful at inference time, but not as much given that requests will come in an asynchronous manner. 
          At that point, it becomes a problem of batching and scheduling. </p>

        <h3>Hardware-aware computing</h3>
        <p>Nowadays, a lot of software is developed without taking any target hardware into account. In spite of this, for certain applications, there are plenty of benefits of knowing 
          how the hardware your application is going to run on works.</p>

        <p>You might recognize one of the Mamba authors, Tri Dao, by his work on FlashAttention todo cite. FlashAttention is an implementation of the attention mechanism that drastically reduces the amount
          of memory transfers between High-Bandwidth Memory and SRAM, from where the processor units pull the data for performing operations. They way this is done is out of the scope of this post, but the 
          main idea is that because Attention is memory-bound (more time moving things around
          than actually doing computations), the less memory transfers, the faster the model. This is a hardware-aware implementation.</p>
          
        <p>The authors recognised that Mamba was memory-bound and did the same: reduce the amount of memory transfers. The usual flow for a GPU program is:
          <ol>
            <li>Load data from HBM to SRAM.</li>
            <li>Compute.</li>
            <li>Store results from SRAM to HBM.</li>
          </ol>
          A naive implementation of an SSM block would discretize, update the hidden state, and then the output, all in separate steps (multiple loops of the flow above). Instead, we discretize, 
          update the hidden state, and compute the output all in one loop. This is called <span class="italic">kernel fusion</span>, a fancy word for combining operations in a single GPU program.
        </p>

        <p>Another technique that they use to make the selective SSM block efficient is <span class="italic">recomputation</span>, which means to avoid storing objects that are
          needed later on (such in the backward pass) in memory and instead compute them again. When an operation is clearly memory-bound, this makes everything go faster.</p>

        <p>In conclusion, the selective SSM block is implemented efficiently mainly by:
          <ul>
            <li>Parallelising the scan operation with the Blelloch algorithm.</li>
            <li>Using hardware-aware computing techniques such as kernel fusion and recomputation.</li>
          </ul>
          
          Including more details would be repeating what's said in the paper, so if you're not sated, please refer to it.</p>

          <h3>The Mamba block</h3>
          <p>Just as we described in <a href="../ssm_2_networks/index.html">post 2</a>, the SSM block is not the only ingredient for a succesful model.
          We saw how one could combine multiple SSM blocks with traditional operations such as softmax or linear projections to build a "neural network"
          with SSMs as a core. Mamba is no different: in the paper, the selective SSM block is combined with multiple other operations to form a complete Mamba block.
        
        todo fig</p>

    </section>

    <section id="A quick Mamba 2">
      <h2>A quick Mamba 2</h2>

      <p>If you have read through all three posts (thank you so much!), you might have noticed that this journey is one of improvements. 
        From plain vanilla state space models to deep networks, through state history reconstruction and hardware-specific optimizations to selection mechanisms,
      we have been incrementally building upon the previous iteration. Mamba 1 and Mamba 2 (todo cite) were published in relatively quick succession, and although
      the practical changes introduced by Mamba 2 were not many, a rich theoretical framework was presented along it. This framework enabled the authors to 
      characterize the relation of SSMs to attention. So, SSMs can be viewed through many different lenses: as continuous, recurrent, convolutional and attention-based models!</p>

      <p>In this section, I want to very briefly summarise the practical changes and the theoretical conclusions presented by the Mamba 2 paper. 
        The theoretical framework is in my opinion the most important result from the paper, and because this series focuses more on the models, I will be skimming through.
        Visit the great Mamba 2 posts (todo ref) for the best in-depth presentation of the topic. In spite of all this, though, we need just a bit of theoretical intuition to contextualise 
        the practical results.</p>

      <h3>Overview of the State Space Duality framework</h3>
      <p>
        Before deriving the relationships between attention and state space models, the authors explore the properties of both from explore generalisations from 
        the perspective of <span class="italic">structured matrices</span> (they both worked with them at the beggining of their PhD's, so it's a natural choice):
        <ul>
          <li>as for <span class="bold">Structured State Space Models</span>, they show an equivalence with semiseparable matrices. 
            From this formulation, the authors show that SSMs can be computed in two ways: a linear, recurrent one via tensor contractions (as usual), and a quadratic (naive) one via pure matrix multiplications.</li>
          <li>as for <span class="bold">attention variants</span>, they define <span class="italic">structured mask attention</span>, which is another
            generalisation. With this, they show that quadratic (the usual) and linear attention are simply different orders by which to compute masked attention via tensor contractions.</li>
        </ul>
      </p>
      <p>
        So, for both from a SSM and an attention perspective, we now have the same two ways to compute them: a linear and a quadratic one. In fact, both turn out to be different perspectives on 
        the same duality. When reducing on a special case, attention and SSMs have the <span class="bold">exact</span> same linear and quadratic forms, and are thus equivalent! 
        This is the Structured State Space Duality (SSD). As for the special cases, they are the following:
        <ul>
          <li>as for SSMs, when $\mathbf{A}$ is a scalar times identity matrix (all elements in the diagonal are the same).</li>
          <li>as for attention, when it's masked and the mask is what they call a 1-semiseparable matrix. We are not that interested in this instantiation because we only implement the SSM case. </li>
        </ul>
      </p>

      <p>
        Let's see what this duality actually implies for our current SSM implementation.
      </p>

      <h3>The consequences of State Space Duality</h3>
      <p>
        Motivated by their strong theoretical findings, the authors went on and implemented the special SSM case (scalar-identity $\mathbf{A}$ matrix) as the new core of Mamba 2. They now call this
        core the SSD model, and plays the same role as an SSM block: can be stacked and combined with other operations to create a neural network. Another change, not in the SSD model itself, 
        but in how its used to deal with inputs $x_t$ with dimension $\gt 1$: use the same SSD model (same $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ matrices}) for each dimension.
      </p>

      <p>The two changes just described allow the SSD model to be expressed in its quadratic form by following the SSD framework. The SSD framework also allows for the creation of a new
        algorithm to compute the model: what they call the SSD algorithm. It uses the FLOPS of SSMs (scales quadratically with input dimension, not sequence length as does Attention), and 
        expresses the model as matrix multiplications just like Attention, thus being more hardware-efficient than SSMs. For completion, two more changes
        changes introduced in Mamba 2 w.r.t Mamba 1 that we have not discussed are:
        <ul>
          <li>the $\mathbf{A}$, $\mathbf{X}$ (embedded), $\mathbf{B}$ and $\mathbf{C}$ matrices are produced by a single projection at the beginning of the SSD block instead of as a function of
            $\mathbf{X}$ (in the case of $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$). This enables better Tensor Parallelism (todo cite).</li>
          <li>added a normalisation layer before the last linear projection of the block, which increases stability.</li>
        </ul>
      </p>

      <p>
        So, all of the theoretical developments have allowed us to develop a model that can be computed much more efficiently, both in memory and compute terms, 
        than Mamba 1, with (approximately) the same predictive performance. A "restriction" of the original Mamba hhas allowed it to train up to 50% quicker and 
        have 8 times bigger states given the same memory footprint. That's Mamba 2.
      </p>
    </section>

    <section id="Series conclusion & references">
      <h2>Series conclusion</h2>
      <p>And this is the end of the SSM series! What I personally find most interesting about the Mamba 2 paper is that it showcases the depth and breadth of skills of the authors: first, they introduce a new theoretical framework, 
        then they apply it to create a new algorithm, and finally they create a whole bunch of optimizations for real world training and inference systems.
      </p>
      
      <p>
        During this series, I have introduced a lot of concepts, and I hope that you have been able to follow along. Of course, what I have presented is just a narrow view of the state and 
        evolution of the field, with many interesting papers and ideas left out. If this has sparked your interest, I recommend that you keep up with the latest research in the field: 
        I have a hunch that the theoretical framework introduced in Mamba 2 will be used in many interesting ways, especially with how it unifies SSMs and attention. The biggest question for me is
        what (and if) other models can be unified with this framework, and what algorithmic discoveries will it bring. The field of deep learning is notorious for introducing 
        innovations that are not really understood from a theoretical perspective, and I think that the SSD framework is a step in the right direction.
      </p>
      
      <p>
        Writing this has been a blast. I hope you have enjoyed reading it as much as I have enjoyed writing it. If you did, sharing it would mean a lot to me. 
        If you have any questions, comments, or suggestions, please don't hesitate to reach out. Until next time!
      </p>

      <h3>References</h3>
      <p><ul id="reference-list"></ul></p>

    </section>

  </article>

  </div>

  <script>

    // -------------------------------------------------- References --------------------------------------------------
    const references = [
      {id: "attention", author: "Vaswani et al", year: "2017", title: "Attention Is All You Need", url: "https://arxiv.org/abs/1706.03762"}
    ];
    
    document.addEventListener('DOMContentLoaded', function() {
      const refList = document.getElementById('reference-list');
      references.forEach(ref => {
          const listItem = document.createElement('li');
          listItem.id = ref.id;
          listItem.innerHTML = `${ref.author}. (${ref.year}). <i>${ref.title}</i>. <a href="${ref.url}">link</a>`;
          refList.appendChild(listItem);
      });
    });
    </script>

</body>

</html>
